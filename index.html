<!DOCTYPE html>
<!-- saved from url=(0028)https://a-nagrani.github.io/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>SAINITHIN ARTHAM</title>
  
  <meta name="author" content="SAINITHIN ARTHAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="cSS.css">
  <link rel="icon" type="image/png" href="https://a-nagrani.github.io/images/seal_icon.png">
</head>

<body data-new-gr-c-s-check-loaded="14.1185.0" data-gr-ext-installed="">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>SAINITHIN ARTHAM</name>
		<br>
		 <a href="mailto:sainithin.artham@bits-pilani.ac.in" ,="" style="color:grey">sai artham at bits-pilani ac in</a>
		<br>
		<br>
              </p>
              <p>I am a Research Fellow  at <a href="https://ai.google/research/">BITS Pilani</a>, where I work on machine learning and deep learning for cyper physical systems. 
		    </p>
		    <p>
              Before that I did my undergrad at the <a href="https://www.bmu.edu.in/">BML Munjal University</a>, where I worked with <a href="https://scholar.google.co.in/citations?user=8nues8UAAAAJ&hl=en">Soharab Hossain Shaikh</a> and <a href="https://scholar.google.co.in/citations?user=a3mzDVYAAAAJ&hl=en">Arijit Maitra</a>.
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="https://drive.google.com/file/d/1n_7QWQEJ1PTLbBytVS0g7mFpHRFsyCZ5/view?usp=sharing">CV</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=3ayWBnwAAAAJ&hl=en&amp;hl=en&amp;oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/sai-artham-89a188207/?originalSubdomain=in"> LinkedIn </a> &nbsp;/&nbsp;
		<a href="https://twitter.com/SainithinA">Twitter</a> &nbsp;/&nbsp;
    <a href="https://github.com/Sainithin-bit"> GitHub </a> &nbsp;/&nbsp;
    <!-- <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20e/nagrani20e.pdf"> Thesis </a> -->
    
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="image.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="image.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!---
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2020
                  <ul>

                      <li>Submissions now open for the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">VoxCeleb Speaker Recognition Challenge</a> at Interspeech 2020!</li>
                      <li>I am spending the summer as a Visiting Researcher at <a href="https://www.wadhwaniai.org/">Wadhwani AI</a>, a non-profit in Mumbai that develop AI solutions for social good.</li>
                      <li>Submissions open for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020! Baselines, code and pre-extracted features provided.</li>
                      <li>Submissions open for the <a href="http://sightsound.org/">Sight and Sound</a> workshop at CVPR 2020!</li>
                      <li>I am once again co-organizing the <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/"> Women in Computer Vision workshop </a> at CVPR 2020 in Seattle.</li>
                  </ul>
                2019 
                  <ul>
                      <li>Our <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">work</a> on Chimpanzee Face Recognition was featured in the <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/">New Scientist</a> and the 
                        <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/">MIT Tech Review</a>. </li>
                      <li>We held the first ever <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html"> VoxCeleb Speaker Recognition Challenge </a> at Interspeech 2019 in Graz, Austria! Over 50 teams participated.</li>
                      <li>I helped organise the <a href="https://wicvworkshop.github.io/CVPR2019/index.html"> Women in Computer Vision workshop </a> at CVPR 2019 in Long Beach, Ca.</li>
                      <li>I am spending the summer at <a href="https://ai.google/research/">Google AI Research</a> in California working with <a href="http://chensun.me/">Chen Sun </a> and <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>.</li>

                  </ul>
                2017
                <ul>
                    <li> I received a <a href="https://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">Google PhD Fellowship</a>.</li>
                    <li> <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">Our paper</a> won the <a href="https://www.isca-speech.org/iscaweb/index.php/honors/awards">Best Student Paper Award</a> at Interspeech 2017! </li>
                    
                </ul>

              </p>
            </td>
          </tr>
-->
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>
        
            <p>
      
              I am  interested in computer vision and have also worked  in vision-language integration. Currently, I am focused on developing vision-language-action models, leveraging advanced techniques in object detection, dense video captioning, and action generation to create systems that can interpret and act upon visual data with a high degree of accuracy and context-awareness.
              For a full list of publications please see <a href="https://scholar.google.com/citations?user=3ayWBnwAAAAJ&hl=en&amp;hl=en&amp;oi=ao">Google Scholar</a>.
      
            </p>
          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="Sainithin Artham Files/Densecap.png" alt="Dense Captioning Model " width="160" height="80">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://link.springer.com/article/10.1007/s11042-023-17809-1">
                    <papertitle>  A Neural ODE and Transformerâ€‘based Model for Temporal Understanding and Dense Video Captioning </papertitle>
                  </a>
                  <br>
			 <b>Sainithin Artham</b>, Soharab Hossain Shaikh

                  <br>
                  <em>Multimedia Tools and Applications</em>, 2024 &nbsp; 
                  <br>

              <a href="https://link.springer.com/article/10.1007/s11042-023-17809-1">arXiv</a> / <a href="https://github.com/Sainithin-bit/Densecap_LTC?tab=readme-ov-file">code, models, data, project page</a>
                  <p></p>
                  <p> Dense video captioning with Neural ODE </p>
                </td>
              </tr>



		<tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src="Sainithin Artham Files/Pred_AHCP.jpg" alt="Dense Captioning Model " width="160" height="80">
                    </td>
                    <td width="75%" valign="middle">
                      <a href="https://www.biorxiv.org/content/10.1101/2024.05.05.592323v1.full.pdf">
                        <papertitle>  Pred-AHCP: Robust feature selection enabled Sequence Specific Prediction of Anti-Hepatitis C Peptides via Machine Learning </papertitle>
                      </a>
                      <br>
                      Akash Saraswat, Utsav Sharma, Aryan Gandotra, Lakshit Wasan, <b>Sainithin Artham</b>, Arijit Maitra, Bipin Singh
           
    
                      <br>
                      <em>Journal of Chemical Information and Modeling</em>, 2024 &nbsp; 
                      <br>
    
                  <a href="https://www.biorxiv.org/content/10.1101/2024.05.05.592323v1.full.pdf">arXiv</a> / <a href="http://15.206.79.148/">code, models, data, project page</a>
                      <p></p>
                      <p> We developed an explainable ML model that harnesses the amino acid sequence of a peptide to predict its potential as an anti-HepC (AHC) agent </p>
                    </td>
                  </tr>
    
    
    
        <tr>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <img src="Sainithin Artham Files/Autonumos Veichiles.jpg" alt="Dense Captioning Model " width="160" height="80">
                        </td>
                        <td width="75%" valign="middle">
                          <a href="https://www.researchsquare.com/article/rs-3506149/v1">
                            <papertitle>  Deep Learning for Autonomous Vehicle Object Detection   </papertitle>
                          </a>
                          <br>
                          <b>Sainithin Artham</b>, Swarali Borde , Shashank Shekhar


               
        
                          <br>
                          <em>IEEE IATSM Coference</em>, 2024 &nbsp; 
                          <br>
        
                      <a href="https://assets-eu.researchsquare.com/files/rs-3506149/v1_covered_35bf40f8-9100-4b64-b078-a325819d3252.pdf?c=1698635824">arXiv</a> 
                          <p></p>
                          <p> This research explores deep neural networks (VGG16, AlexNet,and GoogleNet) for object classification and detection in autonomousvehicles</p>
                        </td>
                      </tr>
        
        
        
            <tr>
              
       
<!--- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching/Invited Talks</heading>
            <p>
            "Multimodality for Video Understanding", <a href="https://sites.google.com/view/aisummerschool2020">Google Research India AI Summer School</a>, 2020 [<a href="https://docs.google.com/presentation/d/1J4AVI9S1KBFZyGiBdFJn64k2BRFhlRRngd8r262Xo8g/edit?usp=sharing">slides</a>] </br>
            "Learning joint representations for visual and language tasks", <a href="https://multimodal-knowledge-discovery.github.io/">Online Multimodal Knowledge Discovery Tutorial</a>, ICDM 2020
            "Applications of Machine Learning", Oxford University MPLS DTC on Statistics and Data Mining, 2020 [<a href="https://docs.google.com/presentation/d/1XgNYFLNwlODst7LiFuaof2F9uuNcv3ZdwdiyXrCT1-A/edit?usp=sharing">slides</a>]
            </p>
          </td>
        </tr>
      </tbody></table>
 --> 
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
	<tr>
              <td width:100%;vertical-align:middle"="">
	<strong>  Area Chair </strong>: CVPR23, ICCV23
	<br>
	<strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access	
          </td>
            </tr>
				   
          <tr>
	
       <td width="100%" valign="center">
	 <strong> Workshop/Tutorial Organization </strong>: 
	<br>
	<br>
	<strong> Sight and Sound </strong> Workshop @ CVPR 
        <br>
        [2020-2022]
	<a href="https://sightsound.org/">website</a> 												              
       <br>
	 <br>
        <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        <br>
        [2021]
<a href="https://arxiv.org/pdf/2201.04583.pdf">report</a> / 	
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2021.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       <br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       <br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               <br>										 
	      <br>
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            <br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            <br>										 
            <br>
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      <br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      <br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         <br>										 
        <br>
        </td>
        </tr>							 -->
								
								
<!-- 	
          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/sightsound.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> Sight and Sound </strong> Workshop @ CVPR 
        </br>
        [2020-2022]
	<a href="https://sightsound.org/">website</a> 												              
       </br>
        </td>
        </tr>
          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/voxsrc.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        </br>
        [2021]
<a href="https://arxiv.org/pdf/2201.04583.pdf">report</a> / 	
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2021.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               </br>										 
	      </br>

        </td>
        </tr>
          <tr>
            <th style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                      <img src="images/video-pent-logo.svg" alt="video-pent" width="100" height="60" class="center">
            </th>
           <td width="75%" valign="center">
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            </br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            </br>										 
            </br>
    
            </td>
            </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/wicv.png" alt="wicv" width="150" height="50">
      </td>
      <td width="75%" valign="center">
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      </br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      </br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         </br>										 
        </br>
        </td>
        </tr> -->
<!--         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                <img src="images/reviewer.jpg" alt="review" width="150" height="70" >
      </td>
          <td>
              <strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access
              <br>
		 <strong>  Area Chair </strong>: CVPR23, ICCV23

            </td>
          </tr> -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>



</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>